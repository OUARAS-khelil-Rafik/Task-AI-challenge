{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2264959a",
   "metadata": {},
   "source": [
    "# 0) Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06fd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q lightgbm dask \"dask[distributed]\" dask-ml tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9036d",
   "metadata": {},
   "source": [
    "# 1) Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34bc59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d167c",
   "metadata": {},
   "source": [
    "# 2) Paths & Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f05683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Paths\n",
    "# -----------------------\n",
    "DATA_DIR = Path('data')\n",
    "TRAIN_CSV = DATA_DIR / 'ctr_train.csv'\n",
    "TEST_CSV = DATA_DIR / 'ctr_test.csv'\n",
    "SAMPLE_SUB = DATA_DIR / 'ctr_sample_submission.csv'\n",
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TARGET = 'click'\n",
    "ID_COL = 'id'\n",
    "IDX_COL = 'idx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d25ee",
   "metadata": {},
   "source": [
    "# 3) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'inproc://192.168.100.2/20728/1' processes=2 threads=4, memory=11.18 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 00:44:08,484 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.57 GiB -- Worker memory limit: 5.59 GiB\n",
      "2025-08-13 00:44:09,540 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 3.69 GiB -- Worker memory limit: 5.59 GiB\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Dask Cluster Setup\n",
    "# -----------------------\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,              # Adjust to your CPU cores\n",
    "    threads_per_worker=2,\n",
    "    processes=False,\n",
    "    memory_limit=\"6GB\",       # 6GB per worker, safe for your 20GB RAM\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c7ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Train partitions (before): 98\n",
      "Loading test data...\n",
      "Test partitions (before): 1\n",
      "Train partitions (after): 882\n",
      "Test partitions (after): 1\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Load Data with Dask\n",
    "# -----------------------\n",
    "print(\"Loading training data...\")\n",
    "train = dd.read_csv(TRAIN_CSV, assume_missing=True, blocksize=\"64MB\")\n",
    "print(f\"Train partitions (before): {train.npartitions}\")\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test = dd.read_csv(TEST_CSV, assume_missing=True, blocksize=\"64MB\")\n",
    "print(f\"Test partitions (before): {test.npartitions}\")\n",
    "\n",
    "# Repartition to smaller chunks (better parallelism)\n",
    "train = train.repartition(partition_size=\"32MB\")\n",
    "test = test.repartition(partition_size=\"32MB\")\n",
    "print(f\"Train partitions (after): {train.npartitions}\")\n",
    "print(f\"Test partitions (after): {test.npartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b51f7594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to category: 100%|██████████| 22/22 [00:01<00:00, 13.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Feature Preparation\n",
    "# -----------------------\n",
    "print(\"Preparing features...\")\n",
    "categorical_cols = [c for c in train.columns if c not in [TARGET, ID_COL, IDX_COL]]\n",
    "\n",
    "# Convert to category to save memory\n",
    "for col in tqdm(categorical_cols, desc=\"Converting to category\"):\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Train/Validation Split\n",
    "# -----------------------\n",
    "\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X = train[categorical_cols]\n",
    "y = train[TARGET]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Compute only when ready to train\n",
    "X_train, X_valid, y_train, y_valid = (\n",
    "    X_train.compute(),\n",
    "    X_valid.compute(),\n",
    "    y_train.compute(),\n",
    "    y_valid.compute(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# LightGBM Dataset\n",
    "# -----------------------\n",
    "dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_cols)\n",
    "dvalid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_cols)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'max_bin': 255,\n",
    "    'verbose': -1,\n",
    "    'device': 'gpu' if lgb.has_gpu() else 'cpu'\n",
    "}\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=200,\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Validation AUC & ROC Curve\n",
    "# -----------------------\n",
    "print(\"Evaluating on validation set...\")\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f\"Validation ROC-AUC: {auc_score:.5f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_valid, y_pred_valid)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.5f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "roc_path = OUTPUT_DIR / \"roc_curve.png\"\n",
    "plt.savefig(roc_path)\n",
    "plt.close()\n",
    "print(f\"ROC curve saved to {roc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Save Model\n",
    "# -----------------------\n",
    "model_path = OUTPUT_DIR / \"lgbm_ctr_model.txt\"\n",
    "model.save_model(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a467a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Predictions for Submission\n",
    "# -----------------------\n",
    "print(\"Predicting on test set...\")\n",
    "test_df = test.compute()\n",
    "y_pred_test = model.predict(test_df[categorical_cols])\n",
    "\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB)\n",
    "sample_sub['click'] = y_pred_test\n",
    "submission_path = OUTPUT_DIR / \"ctr_submission.csv\"\n",
    "sample_sub.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Submission saved to {submission_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
